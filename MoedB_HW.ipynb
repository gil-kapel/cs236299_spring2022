{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!cp -r https://github.com/gil-kapel/cs236299_spring2022/* .\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "almngNDF4aMs",
        "outputId": "e3b746b4-e709-4795-b9d6-e95a76c7db2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-69ea5071b3ff>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ls requirements.txt >/dev/null 2>&1\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "jnNc6FRAsiZD",
        "outputId": "5c5c95ca-ea67-4b94-8ef8-56796c604a11"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b65325337a97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wget'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import itertools\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import wget\n",
        "import torchtext.legacy as tt\n",
        "import copy\n",
        "# import torch\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from sys import getsizeof\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds\n",
        "SEED = 305323776\n",
        "random.seed(SEED)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print (device)\n"
      ],
      "metadata": {
        "id": "qX7FaPWntiTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some utilities to manipulate the corpus - Was taken from lab 2-1\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Strips #comments and empty lines from a string\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for line in text.split(\"\\n\"):\n",
        "        line = line.strip()              # trim whitespace\n",
        "        line = re.sub('#.*$', '', line)  # trim comments\n",
        "        if line != '':                   # drop blank lines\n",
        "            result.append(line)\n",
        "    return result\n",
        "\n",
        "def geah_tokenize(lines):\n",
        "    \"\"\"Specialized tokenizer for GEaH corpus handling speaker IDs\"\"\"\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        # tokenize\n",
        "        tokens = tt.data.get_tokenizer(\"basic_english\")(line)\n",
        "        # revert the speaker ID token\n",
        "        if tokens[0] == \"sam\":\n",
        "            tokens[0] = \"SAM:\"\n",
        "        elif tokens[0] == \"guy\":\n",
        "            tokens[0] = \"GUY:\"\n",
        "        else:\n",
        "            raise ValueError(\"format problem - bad speaker ID\")\n",
        "        # add a start of sentence token\n",
        "        result += [\"<s>\"] + tokens\n",
        "    return result\n",
        "                    \n",
        "def postprocess(tokens):\n",
        "    \"\"\"Converts `tokens` to a string with one sentence per line\"\"\"\n",
        "    return ' '.join(tokens)\\\n",
        "              .replace(\"<s> \", \"\\n\")\n",
        "\n",
        "def split(list, portions, offset):\n",
        "    \"\"\"Splits `list` into a \"large\" and a \"small\" part, returning them as a pair.\n",
        "    \n",
        "    The two parts are formed by partitioning `list` into `portions` disjoint pieces.\n",
        "    The small part is the piece at index `offset`; the large part is the remainder.\n",
        "    \"\"\"\n",
        "    return ([list[i] for i in range(0, len(list)) if i % portions != offset],\n",
        "            [list[i] for i in range(0, len(list)) if i % portions == offset])\n",
        "    "
      ],
      "metadata": {
        "id": "poFB_B6ZtfGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the GEaH data and preprocess into training and test streams of tokens\n",
        "geah_filename = (\"https://github.com/khushmeeet/potter-nlp/blob/master/data/\"\n",
        "                 \"Book 1 - The Philosopher's Stone_djvu.txt.html\")\n",
        "os.makedirs('data', exist_ok=True)\n",
        "wget.download(geah_filename, out=\"data/\")\n",
        "\n",
        "with open(\"data/Book 1 - The Philosopher's Stone_djvu.txt.html\", 'r') as fin:\n",
        "    lines = preprocess(fin.read())\n",
        "    train_lines, test_lines = split(lines, 12, 0)\n",
        "    train_tokens = geah_tokenize(train_lines)\n",
        "    test_tokens = geah_tokenize(test_lines)\n",
        "\n",
        "vocabulary = list(set(train_tokens))\n"
      ],
      "metadata": {
        "id": "JtWrbe70t9GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def all_ngrams(vocabulary, n):\n",
        "    \"\"\"Returns a list of all `n`-long *tuples* of elements of the `vocabulary`.\n",
        "    \"\"\"\n",
        "    return list(itertools.product(vocabulary, repeat = n))\n",
        "\n",
        "def ngrams(tokens, n):\n",
        "    \"\"\"Returns a list of all `n`-gram instances in a list of `tokens`, in order.\n",
        "    \"\"\"\n",
        "    return [tuple(tokens[i : i + n])\n",
        "            for i in range(0, len(tokens) - n + 1)]\n",
        "\n",
        "def ngram_counts(vocabulary, tokens, n):\n",
        "    \"\"\"Returns a dictionary of counts of the `n`-grams in `tokens`.\n",
        "    \n",
        "    The dictionary is structured with first index by (n-1)-gram context\n",
        "    and second index by the final target token.\n",
        "    \"\"\"\n",
        "    context_dict = defaultdict(lambda: defaultdict(int))\n",
        "    # zero all ngrams\n",
        "    for context in all_ngrams(vocabulary, n - 1):\n",
        "        for target in vocabulary:\n",
        "            context_dict[context][target] = 0\n",
        "    # add counts for attested ngrams\n",
        "    for ngram, count in Counter(ngrams(tokens, n)).items():\n",
        "        context_dict[ngram[:-1]][ngram[-1]] = count\n",
        "    return context_dict"
      ],
      "metadata": {
        "id": "PKF7XMsLuQSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_model(ngram_counts):\n",
        "    \"\"\"Returns an n-gram probability model calculated by normalizing the \n",
        "       provided `ngram-counts` dictionary\n",
        "    \"\"\"\n",
        "    import copy\n",
        "    normalized = copy.deepcopy(ngram_counts)\n",
        "    for key, value in ngram_counts.items():\n",
        "      for word in value:\n",
        "        s = sum(value.values())\n",
        "        if s == 0:\n",
        "          normalized[key][word] = 0\n",
        "        else:\n",
        "          normalized[key][word] = normalized[key][word] / s\n",
        "    return normalized\n"
      ],
      "metadata": {
        "id": "rQe6uejwu0tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, context):\n",
        "    \"\"\"Returns a token sampled from the `model` assuming the `context`\"\"\"\n",
        "    distribution = model[context]\n",
        "    prob_remaining = random.random()\n",
        "    for token, prob in sorted(distribution.items()):\n",
        "        if prob_remaining < prob:\n",
        "            return token\n",
        "        else:\n",
        "            prob_remaining -= prob\n",
        "    raise ValueError\n",
        "\n",
        "def sample_sequence(model, start_context, count=100):\n",
        "    \"\"\"Returns a sequence of `count` tokens sampled successively\n",
        "       from the `model` *following the `start_context`*.\n",
        "       The length of the returned list should be `count+len(start_context)`.\n",
        "    \"\"\"\n",
        "    random.seed(SEED) # for reproducibility, do not change\n",
        "    seq = list(start_context)\n",
        "    n = len(start_context)\n",
        "    for i in range(n+1, count+n+1):\n",
        "      next_context = seq[i-(n+1) : i]\n",
        "      next_word = sample(model, tuple(next_context))\n",
        "      seq.append(next_word)\n",
        "    return seq\n"
      ],
      "metadata": {
        "id": "nKlkHMDxuTam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def probability(tokens, model, n):\n",
        "    \"\"\"Returns the probability of a sequence of `tokens` according to an\n",
        "       `n`-gram `model`\n",
        "    \"\"\"\n",
        "    score = 1.0\n",
        "    context = tokens[0:n-1]\n",
        "    # Ignores the scores of the first n-1 tokens\n",
        "    for token in tokens[n-1:]:\n",
        "        prob = model[tuple(context)][token]\n",
        "        score *= prob\n",
        "        context = (context + [token])[1:]\n",
        "    return score\n",
        "\n",
        "def neglogprob(tokens, model, n):\n",
        "    \"\"\"Returns the negative log probability of a sequence of `tokens`\n",
        "       according to an `n`-gram `model`\n",
        "    \"\"\"\n",
        "    score = probability(tokens, model, n)\n",
        "    if score == 0:\n",
        "      return math.inf\n",
        "    else:\n",
        "      return -math.log2(score)\n",
        "\n",
        "def perplexity(tokens, model, n):\n",
        "    \"\"\"Returns the perplexity of a sequence of `tokens` according to an\n",
        "       `n`-gram `model`\n",
        "    \"\"\"\n",
        "    N = len(tokens) - n + 1\n",
        "    prob = 2 ** (-neglogprob(tokens, model, n))\n",
        "    if prob == 0:\n",
        "      return math.inf\n",
        "    else:\n",
        "      return (1 / prob) ** (1/N)"
      ],
      "metadata": {
        "id": "TsUsTG_EvcVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_model_smoothed(ngram_counts, delta:float):\n",
        "  normalized = copy.deepcopy(ngram_counts)\n",
        "  for key, value in ngram_counts.items():\n",
        "    for word in value:\n",
        "      s = sum(value.values())\n",
        "      normalized[key][word] = (normalized[key][word] + delta) / (s + len(normalized)*delta)\n",
        "  return normalized"
      ],
      "metadata": {
        "id": "29tgaM2TvpWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_counts = ngram_counts(vocabulary, train_tokens, 1)\n",
        "bigram_counts = ngram_counts(vocabulary, train_tokens, 2)\n",
        "trigram_counts = ngram_counts(vocabulary, train_tokens, 3)\n",
        "\n",
        "unigram_model = ngram_model(unigram_counts)\n",
        "bigram_model = ngram_model(bigram_counts)\n",
        "trigram_model = ngram_model(trigram_counts)\n",
        "\n",
        "delta = 1\n",
        "\n",
        "unigram_model_smoothed = ngram_model_smoothed(copy.deepcopy(unigram_counts), delta)\n",
        "bigram_model_smoothed = ngram_model_smoothed(copy.deepcopy(bigram_counts), delta)\n",
        "trigram_model_smoothed = ngram_model_smoothed(copy.deepcopy(trigram_counts), delta)\n",
        "\n",
        "print(f\"Test perplexity - smoothed unigram: {perplexity(test_tokens, unigram_model_smoothed, 1):.3f}\\n\"\n",
        "      f\"Test perplexity - smoothed bigram: {perplexity(test_tokens, bigram_model_smoothed, 2):.3f}\\n\"\n",
        "      f\"Test perplexity - smoothed trigram: {perplexity(test_tokens, trigram_model_smoothed, 3):.3f}\")\n",
        "\n",
        "print(f\"Test perplexity - unigram: {perplexity(test_tokens, unigram_model, 1):.3f}\\n\"\n",
        "      f\"Test perplexity -  bigram: {perplexity(test_tokens, bigram_model, 2):.3f}\\n\"\n",
        "      f\"Test perplexity - trigram: {perplexity(test_tokens, trigram_model, 3):.3f}\")"
      ],
      "metadata": {
        "id": "B8okN4dEvvYv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}